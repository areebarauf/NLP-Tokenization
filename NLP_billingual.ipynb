{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Code to mount the drive, the folder NLP_2 is made and has all the categories with their articles are saved. ⚡"
      ],
      "metadata": {
        "id": "_u8gfR5WKx8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "CvkihejDxnxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc60e0d-6716-4de6-b8cd-06e7d80f56da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to fetch data from wikipedia, language parameter of the method is to choose the language of the category that is being fetched"
      ],
      "metadata": {
        "id": "FpXCT1PsKeAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def savePage(title,folder, language ='en'):\n",
        "    base_url = 'https://{}.wikipedia.org/w/api.php'.format(language)\n",
        "    response = requests.get(\n",
        "    base_url,\n",
        "    params={\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'titles': title,\n",
        "        'prop': 'extracts',\n",
        "        'explaintext': True,\n",
        "        'language': 'de'\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    text = ''\n",
        "    pagelist = response.get('query').get('pages')\n",
        "    for page in pagelist:\n",
        "        wikipage = pagelist[page]\n",
        "        text = wikipage['extract']\n",
        "        break\n",
        "\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "    f = codecs.open(folder+'/'+title.replace(' ','_')+'.txt','w','utf8')\n",
        "    f.write(text)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "\n",
        "def getWikiPages(category, language= 'en'):\n",
        "    titles = []\n",
        "    base_url = 'https://{}.wikipedia.org/w/api.php'.format(language)\n",
        "    print(base_url)\n",
        "    response = requests.get(\n",
        "        base_url,\n",
        "        params={\n",
        "            'action': 'query',\n",
        "            'list': 'categorymembers',\n",
        "            'cmtitle': 'Category:'+category,\n",
        "            'cmtype':'page',\n",
        "            'format': 'json'\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "\n",
        "    while(response):\n",
        "        titles.extend( [t['title'] for t in response['query']['categorymembers']] )\n",
        "        if 'continue' in response:\n",
        "            cont = response['continue']['cmcontinue']\n",
        "            response = requests.get(\n",
        "                base_url,\n",
        "                params={\n",
        "                    'action': 'query',\n",
        "                    'list': 'categorymembers',\n",
        "                    'cmtitle': 'Category:' + category,\n",
        "                    'cmtype': 'page',\n",
        "                    'cmcontinue': cont,\n",
        "                    'format': 'json'\n",
        "                }\n",
        "            ).json()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        if not cont:\n",
        "            response = None\n",
        "\n",
        "    return titles\n",
        "\n",
        "\n",
        "def downloadWikiCat(category,folder,recursive=False, language= 'en'):\n",
        "    titles = getWikiPages(category, language)\n",
        "    for t in titles:\n",
        "        if r'/' not in t:\n",
        "           savePage(t,folder, language)\n",
        "    return"
      ],
      "metadata": {
        "id": "Kw7j3g0uQq_o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This piece of code calls the above wikipedia api and store the articles under the category chosen for both english and german articles of the similar category"
      ],
      "metadata": {
        "id": "bgGNCnSSKc9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Histroy , Infectious diseases , Technology\n",
        "\n",
        "# wiki_category = 'Technology'\n",
        "# save_folder = '/content/drive/MyDrive/NLP_2/tech_en'\n",
        "\n",
        "# downloadWikiCat(wiki_category, save_folder, language='en')"
      ],
      "metadata": {
        "id": "qpsBAblWQt6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Geschichte , Infektionskrankheit , Technologie\n",
        "\n",
        "# wiki_category = 'Technologie'\n",
        "# save_folder = '/content/drive/MyDrive/NLP_2/tech_de'\n",
        "\n",
        "# downloadWikiCat(wiki_category, save_folder, language='de')"
      ],
      "metadata": {
        "id": "2jNZ3Nr2HAGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQaj740MgMN",
        "outputId": "ca944a1b-9b7b-418b-e6b4-6d8d600d877e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # this gets teh current folder path\n",
        "# current_folder = os.getcwd()\n",
        "\n",
        "# # list of folders to be merged\n",
        "# list_dir = ['disease_de', ' history_de', 'tech_de',\n",
        "#             'disease_en', 'history_en','tech_en']\n",
        "# root_folder = '/content/drive/MyDrive/NLP_2'\n",
        "# output_file = 'combined_files.txt'\n",
        "\n",
        "# # Function to combine files in a folder\n",
        "# def combine_files_in_folder(folder_path):\n",
        "#     combined_data = []\n",
        "#     for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "#         for filename in filenames:\n",
        "#             file_path = os.path.join(dirpath, filename)\n",
        "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#                 combined_data.append(file.read())\n",
        "#     return combined_data\n",
        "\n",
        "# # # Create or open the output file for writing\n",
        "# # with open(output_file, 'w', encoding='utf-8') as output:\n",
        "# #     for sub_folder in os.listdir(root_folder):\n",
        "# #         sub_folder_path = os.path.join(root_folder, sub_folder)\n",
        "# #         if os.path.isdir(sub_folder_path):\n",
        "# #             combined_data = combine_files_in_folder(sub_folder_path)\n",
        "# #             # Write the combined data to the output file\n",
        "# #             output.writelines(combined_data)\n",
        "# #             output.write('\\n')  # Add a separator between folder content\n",
        "\n",
        "# # print(f'Combined files from {root_folder} into {output_file}')\n"
      ],
      "metadata": {
        "id": "8d6uVhYPMgE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_eng_file = '/content/drive/MyDrive/NLP_2/combined_english.txt'\n",
        "# output_ger_file = '/content/drive/MyDrive/NLP_2/combined_german.txt'\n",
        "\n",
        "# with open(output_eng_file, 'w', encoding='utf-8') as output:\n",
        "#   eng_combined = combine_files_in_folder('/content/drive/MyDrive/NLP_2/english')\n",
        "#   output.writelines(eng_combined)\n",
        "\n",
        "# with open(output_ger_file, 'w', encoding='utf-8') as output:\n",
        "#   ger_combined = combine_files_in_folder('/content/drive/MyDrive/NLP_2/german')\n",
        "#   output.writelines(ger_combined)"
      ],
      "metadata": {
        "id": "bFOgMU2x66RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining articles of different language got from Wikipedia in one file to preprocess and then make word embeddings"
      ],
      "metadata": {
        "id": "wFTe3HRTWWWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shutil - High-level File Operation Library\n",
        "The shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal."
      ],
      "metadata": {
        "id": "B3PpyHa0NQvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import glob\n",
        "\n",
        "folder_path = \"/content/driver/MyDrive/NLP_2/german/history_de\"\n",
        "\n",
        "# This part actually lists all files with a .txt extension in the specified folder\n",
        "txt_files = glob.glob(f'{folder_path}/*.txt')\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_2/german_corpus', 'wb') as outfile:\n",
        "    for filename in txt_files:\n",
        "        if filename == '/content/drive/MyDrive/NLP_2/german_corpus':\n",
        "            continue\n",
        "        with open(filename, 'rb') as readfile:\n",
        "            shutil.copyfileobj(readfile, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "DAWjMjzOP89e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining the two corpora to perform preprocessing steps and then train the tokens embeddings"
      ],
      "metadata": {
        "id": "3CUXlVQDFUms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filenames = ['/content/drive/MyDrive/NLP_2/german_corpus', '/content/drive/MyDrive/NLP_2/english_corpus']\n",
        "# with open('/content/drive/MyDrive/NLP_2/combined_corpus', 'w') as outfile:\n",
        "#     for fname in filenames:\n",
        "#         with open(fname) as infile:\n",
        "#             for line in infile:\n",
        "#                 outfile.write(line)"
      ],
      "metadata": {
        "id": "LorFDcb8FU3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing steps on bilingual corpus\n",
        "\n",
        "## using text_preprocessing for performing pre-processing steps"
      ],
      "metadata": {
        "id": "RATKFCp_WjyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text_preprocessing"
      ],
      "metadata": {
        "id": "hdRKytizbwLg",
        "outputId": "ed105909-5578-46f3-c5ca-5a01d1768578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting text_preprocessing\n",
            "  Downloading text_preprocessing-0.1.1-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from text_preprocessing) (3.8.1)\n",
            "Collecting pyspellchecker (from text_preprocessing)\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contractions (from text_preprocessing)\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting names-dataset==2.1 (from text_preprocessing)\n",
            "  Downloading names_dataset-2.1.0-py3-none-any.whl (62.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unittest-xml-reporting (from text_preprocessing)\n",
            "  Downloading unittest_xml_reporting-3.2.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions->text_preprocessing)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->text_preprocessing) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->text_preprocessing) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->text_preprocessing) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->text_preprocessing) (4.66.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unittest-xml-reporting->text_preprocessing) (4.9.3)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions->text_preprocessing)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions->text_preprocessing)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: names-dataset, unittest-xml-reporting, pyspellchecker, pyahocorasick, anyascii, textsearch, contractions, text_preprocessing\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 names-dataset-2.1.0 pyahocorasick-2.0.0 pyspellchecker-0.7.2 text_preprocessing-0.1.1 textsearch-0.0.24 unittest-xml-reporting-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess_text\n",
        "from text_preprocessing import to_lower, remove_email, remove_url, remove_punctuation, lemmatize_word, keep_alpha_numeric,tokenize_word, remove_stopword"
      ],
      "metadata": {
        "id": "tyq_PyDmWzzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9f18d5-dd5d-4024-eb4d-bd5d18d795c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### here I am first preprocessing both teh english and german corpus and then making token of the processed text"
      ],
      "metadata": {
        "id": "0haWkPQRS8Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP_2/combined_german.txt') as file:\n",
        "    sentences_file_german = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_2/combined_english.txt') as file:\n",
        "    sentences_file_english = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_2/combined_files.txt') as file:\n",
        "    sentences_file_combined = file.read()\n",
        "#print(sentences_file)\n",
        "# Preprocess text using custom preprocess functions in the pipeline\n",
        "preprocess_functions = [remove_url, remove_stopword]\n",
        "preprocessed_text_german = preprocess_text(sentences_file_german, preprocess_functions)\n",
        "preprocessed_text_english = preprocess_text(sentences_file_english, preprocess_functions)\n",
        "preprocessed_text_combined = preprocess_text(sentences_file_combined, preprocess_functions)"
      ],
      "metadata": {
        "id": "feYiqnwRbgeu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## making tokens of the processed text"
      ],
      "metadata": {
        "id": "eKctbh2SWPZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (word_tokenize, sent_tokenize, TreebankWordDetokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer)\n",
        "\n",
        "word_tokenized_english = []\n",
        "word_tokenized_german = []\n",
        "word_token_combine = []\n",
        "word_tokenized_english.append(wordpunct_tokenize(preprocessed_text_english))\n",
        "word_tokenized_german.append(wordpunct_tokenize(preprocessed_text_german))\n",
        "word_token_combine.append(wordpunct_tokenize(preprocessed_text_combined))"
      ],
      "metadata": {
        "id": "bQJXJHEQcrzv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These german and english token are then passed in the gensim Word2Vec model for generating embeddings with skip-gram model and they are saved individually"
      ],
      "metadata": {
        "id": "LXEzMKC0WeW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Train the Word2Vec model for bilingual embeddings\n",
        "model_german = Word2Vec(sentences=word_tokenized_german, vector_size=200, window=7, min_count=1, sg=1)\n",
        "model_english = Word2Vec(sentences=word_tokenized_english, vector_size=200, window=7, min_count=1, sg=1)\n",
        "model_combine = Word2Vec(sentences=word_token_combine, vector_size=200, window=7, min_count=1, sg=1)\n",
        "# Saving the trained embeddings\n",
        "model_german.save(\"german_word2vec.model\")\n",
        "model_english.save(\"english_word2vec.model\")\n",
        "model_combine.save(\"combined_word2vec.model\")\n",
        "# Storing just the words + their trained embeddings.\n",
        "#german word vector\n",
        "wv_german = model_german.wv\n",
        "wv_german.save(\"german_word2vec.wordvectors\")\n",
        "\n",
        "# english word vector\n",
        "wv_english = model_english.wv\n",
        "wv_english.save(\"english_word2vec.wordvectors\")\n"
      ],
      "metadata": {
        "id": "n_XrTqCZia0c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# english word vector\n",
        "wv_combine = model_combine.wv\n",
        "wv_combine.save(\"combine_word2vec.wordvectors\")"
      ],
      "metadata": {
        "id": "Tnt0-DFXVZuY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now these embeddings needs to be tested in terms of their similarity and relations/ context(translation) in both languages, so Average Precision of word pair(english word, german translation) is listed and evaluated"
      ],
      "metadata": {
        "id": "kpxMY5HNbZtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "## Defining a list of words for evaluation (English and their translations in German)\n",
        "# word_pairs = [(\"disease\", \"Krankheit\"), (\"Mutations\", \"Mutationen\"), (\"used\", \"eingesetzt\")]\n",
        "\n",
        "word_pairs = [(\"disease\", \"Krankheit\"), (\"dates\", \"Daten\"), (\"used\", \"eingesetzt\"), (\"Mexico\", \"Mexiko\")]\n",
        "ap_scores = []\n",
        "\n",
        "# Calculate Average Precision (AP) for each word pair\n",
        "for english_word, german_word in word_pairs:\n",
        "    try:\n",
        "        # Find the most similar terms from L1 (English) for B (German)\n",
        "        similar_terms_from_l1 = [word[0] for word in model_english.wv.most_similar(positive=[model_german.wv[german_word]])]\n",
        "        print(similar_terms_from_l1)\n",
        "\n",
        "        # Calculating AP based on the position of A (English word) in the list\n",
        "        ap = 0.0\n",
        "        num_correct = 0\n",
        "\n",
        "        for i, term in enumerate(similar_terms_from_l1, 1):\n",
        "          term_vector = model_english.wv[term]\n",
        "          eng_vector = model_english.wv[english_word]\n",
        "          cosine_sim = cosine(eng_vector, term_vector)\n",
        "          if cosine_sim >= 0.9:\n",
        "                num_correct += 1\n",
        "                ap += num_correct / i\n",
        "\n",
        "        if num_correct > 0:\n",
        "            ap /= num_correct\n",
        "\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "        print(f\"Word Pair: ({english_word}, {german_word}), AP: {ap:.2f}\")\n",
        "    except KeyError:\n",
        "        print(f\"One or both of the words not found in vocabulary.\")\n",
        "\n",
        "# Calculate Mean Average Precision (MAP)\n",
        "map_score = np.mean(ap_scores)\n",
        "\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.2f}\")"
      ],
      "metadata": {
        "id": "fPMc8NGJmAuy",
        "outputId": "6a33e0b7-cc35-4633-f619-eb03335765d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Toynbee', 'intervals', 'exposed', 'Furet', 'classics', 'Fasolt', 'really', 'instant', 'classificatory', 'chapters']\n",
            "Word Pair: (disease, Krankheit), AP: 1.00\n",
            "['Toynbee', 'intervals', 'exposed', 'Furet', 'classics', 'really', 'instant', 'Fasolt', 'separates', 'untaught']\n",
            "Word Pair: (dates, Daten), AP: 0.00\n",
            "['Toynbee', 'exposed', 'intervals', 'Fasolt', 'separates', 'Furet', 'classics', 'really', 'classificatory', 'instant']\n",
            "Word Pair: (used, eingesetzt), AP: 0.00\n",
            "['Toynbee', 'exposed', 'intervals', 'Fasolt', 'Furet', 'classificatory', 'classics', 'separates', 'floruit', 'chapters']\n",
            "Word Pair: (Mexico, Mexiko), AP: 0.00\n",
            "Mean Average Precision (MAP): 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "## Defining a list of words for evaluation (English and their translations in German)\n",
        "# word_pairs = [(\"disease\", \"Krankheit\"), (\"Mutations\", \"Mutationen\"), (\"used\", \"eingesetzt\")]\n",
        "\n",
        "word_pairs = [(\"disease\", \"Krankheit\"),\n",
        "    (\"dates\", \"Daten\"),\n",
        "    (\"used\", \"eingesetzt\"),\n",
        "    (\"Mexico\", \"Mexiko\"),\n",
        "    (\"infection\", \"Infektion\"),\n",
        "    (\"treatment\", \"Behandlung\"),\n",
        "    (\"vaccine\", \"Impfstoff\"),\n",
        "    (\"outbreak\", \"Ausbruch\"),\n",
        "    (\"symptoms\", \"Symptome\"),\n",
        "    (\"immune\", \"immun\"),\n",
        "    (\"diagnosis\", \"Diagnose\"),\n",
        "    (\"prevention\", \"Prävention\"),\n",
        "    (\"hospital\", \"Krankenhaus\")]\n",
        "ap_scores = []\n",
        "\n",
        "# Calculate Average Precision (AP) for each word pair\n",
        "for english_word, german_word in word_pairs:\n",
        "    try:\n",
        "        # Find the most similar terms from L1 (English) for B (German)\n",
        "        similar_terms_from_l1 = [word[0] for word in model_combine.wv.most_similar(positive=[model_combine.wv[german_word]])]\n",
        "        print(similar_terms_from_l1)\n",
        "\n",
        "        # Calculating AP based on the position of A (English word) in the list\n",
        "        ap = 0.0\n",
        "        num_correct = 0\n",
        "\n",
        "        for i, term in enumerate(similar_terms_from_l1, 1):\n",
        "          if term == german_word:\n",
        "            num_correct += 1\n",
        "            ap += num_correct / i\n",
        "\n",
        "        if num_correct > 0:\n",
        "            ap /= num_correct\n",
        "\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "        print(f\"Word Pair: ({english_word}, {german_word}), AP: {ap:.2f}\")\n",
        "    except KeyError:\n",
        "        print(f\"One or both of the words not found in vocabulary.\")\n",
        "\n",
        "# Calculate Mean Average Precision (MAP)\n",
        "map_score = np.mean(ap_scores)\n",
        "\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP3DYDyuODHl",
        "outputId": "1bcf56ef-0df1-4fca-ed1f-b2c4642ce5f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Krankheit', 'Marianne', 'Pflanzenteilen', 'wölbt', 'Metapneumovirus', 'Zerebrospinalflüssigkeit', 'GmbH', 'absondern', 'evoking', 'Barrett']\n",
            "Word Pair: (disease, Krankheit), AP: 1.00\n",
            "['Daten', 'Austro', 'weapon', 'Marzahn', 'unwanted', 'tolerize', 'expounded', 'schwerere', 'exogenen', 'protist']\n",
            "Word Pair: (dates, Daten), AP: 1.00\n",
            "['eingesetzt', 'isoniazid', 'Bohrdrähte', 'verminderten', 'Democritus', '4196', 'spacecraft', 'Könige', 'defecations', 'controversia']\n",
            "Word Pair: (used, eingesetzt), AP: 1.00\n",
            "['Mexiko', 'justification', 'Ornithology', 'Merwin', 'Schiebold', 'Gaimard', 'swollen', 'Concordiae', 'Peck', 'Sehnenscheidenentzündung']\n",
            "Word Pair: (Mexico, Mexiko), AP: 1.00\n",
            "['Infektion', 'Verbrennungsmaschine', 'Vorfall', 'Neurologische', 'Naison', 'odontogen', 'Frühsymptome', 'Ausbrüche', 'Nebenhodens', 'proceeded']\n",
            "Word Pair: (infection, Infektion), AP: 1.00\n",
            "['Behandlung', 'nachweisen', 'Action', 'Browser', 'maltophilia', 'knowing', 'hämatogenen', 'Wandertaubenembryo', 'entails', 'Juliet']\n",
            "Word Pair: (treatment, Behandlung), AP: 1.00\n",
            "['Impfstoff', 'Fluoreszenz', 'Materialien', 'Bild', 'Amanda', 'Curti', 'Gesamtsystems', 'atmospheres', 'Götterwelt', 'considerable']\n",
            "Word Pair: (vaccine, Impfstoff), AP: 1.00\n",
            "['Ausbruch', 'Überresten', 'Orbitalphlegmone', 'Informationsportal', 'Kenneth', 'socio', 'mechanism', 'IOS', 'Helmstedt', '0398071322']\n",
            "Word Pair: (outbreak, Ausbruch), AP: 1.00\n",
            "['Symptome', 'interrelated', 'Tabernacle', 'amore', 'encroaching', 'Giese', 'ihrer', 'Erbinformation', 'vertragen', 'Vermeiden']\n",
            "Word Pair: (symptoms, Symptome), AP: 1.00\n",
            "['immun', 'Habitual', '0398071322', 'promoter', 'co', 'Operational', 'Mintz', 'semi', 'nano', 'Fidelity']\n",
            "Word Pair: (immune, immun), AP: 1.00\n",
            "['Diagnose', 'Schwimmer', 'aboveTechnology', 'Immunosuppressing', 'Pietri', 'freshly', 'prompt', 'Frame', 'comma', 'Ausliefern']\n",
            "Word Pair: (diagnosis, Diagnose), AP: 1.00\n",
            "['Prävention', 'libros', 'mysticism', 'aufzufassen', 'clotting', 'südamerikanischen', 'cognitive', 'collaborations', 'immunologischen', 'Wissenschaftstheorie']\n",
            "Word Pair: (prevention, Prävention), AP: 1.00\n",
            "['Krankenhaus', 'polyarthralgia', 'Archiv', 'variola', 'erneute', 'pathways', 'Wir', 'Forschungsprojekts', 'symbolism', 'Gründerplattform']\n",
            "Word Pair: (hospital, Krankenhaus), AP: 1.00\n",
            "Mean Average Precision (MAP): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying teh above code with more word pairs,"
      ],
      "metadata": {
        "id": "wCQdPOCpiVJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = [\n",
        "    (\"disease\", \"Krankheit\"),\n",
        "    (\"Mexico\", \"Mexiko\"),\n",
        "    (\"used\", \"eingesetzt\"),\n",
        "    (\"infection\", \"Infektion\"),\n",
        "    (\"treatment\", \"Behandlung\"),\n",
        "    (\"vaccine\", \"Impfstoff\"),\n",
        "    (\"outbreak\", \"Ausbruch\"),\n",
        "    (\"symptoms\", \"Symptome\"),\n",
        "    (\"immune\", \"immun\"),\n",
        "    (\"diagnosis\", \"Diagnose\"),\n",
        "    (\"prevention\", \"Prävention\"),\n",
        "    (\"hospital\", \"Krankenhaus\")\n",
        "]\n",
        "\n",
        "for english_word, german_word in word_pairs:\n",
        "    try:\n",
        "        similar_terms_from_l1 = [word[0] for word in model_english.wv.most_similar(positive=[model_german.wv[german_word]])]\n",
        "        ap = 0.0\n",
        "        num_correct = 0\n",
        "\n",
        "        for i, term in enumerate(similar_terms_from_l1, 1):\n",
        "          term_vector = model_english.wv[term]\n",
        "          eng_vector = model_english.wv[english_word]\n",
        "          cosine_sim = cosine(eng_vector, term_vector)\n",
        "          if cosine_sim >= 0.9:\n",
        "                num_correct += 1\n",
        "                ap += num_correct / i\n",
        "\n",
        "        if num_correct > 0:\n",
        "          ap /= num_correct\n",
        "        ap_scores.append(ap)\n",
        "        print(f\"Word Pair: ({english_word}, {german_word}), AP: {ap:.2f}\")\n",
        "    except KeyError:\n",
        "        print(f\"One or both of the words not found in vocabulary.\")\n",
        "\n",
        "# Calculate Mean Average Precision (MAP)\n",
        "map_score = np.mean(ap_scores)\n",
        "\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "792YAHVlbtME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03266673-5271-47b9-9526-6da18b5a8ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Pair: (disease, Krankheit), AP: 1.00\n",
            "Word Pair: (Mexico, Mexiko), AP: 0.00\n",
            "Word Pair: (used, eingesetzt), AP: 0.00\n",
            "Word Pair: (infection, Infektion), AP: 1.00\n",
            "Word Pair: (treatment, Behandlung), AP: 0.64\n",
            "Word Pair: (vaccine, Impfstoff), AP: 1.00\n",
            "Word Pair: (outbreak, Ausbruch), AP: 0.00\n",
            "Word Pair: (symptoms, Symptome), AP: 0.90\n",
            "Word Pair: (immune, immun), AP: 0.96\n",
            "Word Pair: (diagnosis, Diagnose), AP: 0.99\n",
            "Word Pair: (prevention, Prävention), AP: 0.88\n",
            "Word Pair: (hospital, Krankenhaus), AP: 0.47\n",
            "Mean Average Precision (MAP): 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the beginning of my implementation I didn't get any good average precisions, assuming the embeddings are not good enough with the basic token. Here using BPE, Word Piece algorithm for tokenization to better capture the morphological essence in german text. trying above task with new tokenization method and embeddings"
      ],
      "metadata": {
        "id": "TJ5nPxIaN7c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "39cMbLLqQec2",
        "outputId": "03681857-7a8a-42da-e880-00f3aaa40cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<0.18,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub, tokenizers\n",
            "Successfully installed huggingface_hub-0.17.3 tokenizers-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from tokenizers import Tokenizer\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
        "                                WordPieceTrainer, UnigramTrainer\n",
        "\n",
        "## a pretokenizer to segment the text into words\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae8f3hN3N23h",
        "outputId": "c30a15d6-e204-4660-aa51-de6496cc3128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unk_token = \"<UNK>\"  # token for unknown words\n",
        "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
        "\n",
        "def prepare_tokenizer_trainer(alg):\n",
        "    \"\"\"\n",
        "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
        "    \"\"\"\n",
        "    if alg == 'BPE':\n",
        "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
        "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
        "    elif alg == 'WPC':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
        "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
        "\n",
        "\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    tokenizer.stopwords = STOPWORDS\n",
        "    return tokenizer, trainer\n",
        "\n",
        "def tokenize(input_string, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes the input string using the tokenizer provided.\n",
        "    \"\"\"\n",
        "    output = tokenizer.encode(input_string)\n",
        "    return output"
      ],
      "metadata": {
        "id": "pxF_UDyWN26J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(files, alg='BPE'):\n",
        "    \"\"\"\n",
        "    Takes the files and trains the tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
        "    tokenizer.train(files, trainer) # training the tokenzier\n",
        "    tokenizer.save(\"./tokenizer-trained.json\")\n",
        "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "KzArVvX0N28h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating tokens, I have combined english and german text in a single file and then performed preprocessing and used above mentioned algorithms for token creation. Also the same is suggested in the task by the professor|"
      ],
      "metadata": {
        "id": "wWk4HaFdjo-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP_2/combined_files.txt') as file:\n",
        "    sentences_file = file.read().splitlines()\n",
        "\n",
        "preprocessed_text =[]\n",
        "tokenized_sentences = []\n",
        "trained_tokenizer = train_tokenizer(['/content/drive/MyDrive/NLP_2/combined_files.txt'], 'WPC')\n"
      ],
      "metadata": {
        "id": "s9j73wD0OnD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP_2/combined_files.txt') as file:\n",
        "    sentences_file = file.read()\n",
        "\n",
        "\n",
        "preprocessed_text =[]\n",
        "tokenized_sentences = []\n",
        "preprocessed_text =[]\n",
        "\n",
        "# change the variable name to combine\n",
        "preprocess_functions = [remove_punctuation, remove_url, remove_stopword]\n",
        "preprocessed_text_german = preprocess_text(sentences_file, preprocess_functions)\n",
        "encoding_ger = tokenize(preprocessed_text_german,trained_tokenizer)\n",
        "subword_tokens = encoding_ger.tokens\n",
        "tokenized_sentences.append(subword_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "hlF6G7owOnKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=200, window=7, min_count=1, sg=1)\n",
        "\n",
        "# Saving the trained model combined\n",
        "model.save(\"combined_word2vec.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ByRcLagyN2_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "word_pairs = [\n",
        "    (\"disease\", \"Krankheit\"),\n",
        "    (\"Mexico\", \"Mexiko\"),\n",
        "    (\"Bronze\", \"Bronze\"),\n",
        "    (\"humanity\", \"Menschheit\"),\n",
        "    (\"cook\", \"Kochen\")\n",
        "]\n",
        "\n",
        "ap_scores = []\n",
        "dict_1 ={}\n",
        "\n",
        "for english_word, german_word in word_pairs:\n",
        "    try:\n",
        "        similar_terms_from_l1 = [word[0] for word in model.wv.most_similar(negative=[model.wv[german_word]])]\n",
        "        print(similar_terms_from_l1)\n",
        "        ap = 0.0\n",
        "        num_correct = 0\n",
        "\n",
        "        for i, term in enumerate(similar_terms_from_l1, 1):\n",
        "          if term == german_word:\n",
        "                num_correct += 1\n",
        "                ap += num_correct / i\n",
        "\n",
        "        if num_correct > 0:\n",
        "          ap /= num_correct\n",
        "\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "        print(f\"Word Pair: ({english_word}, {german_word}), AP: {ap:.2f}\")\n",
        "    except KeyError:\n",
        "        print(f\"One or both of the words not found in vocabulary.\")\n",
        "\n",
        "map_score = np.mean(ap_scores)\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "XxVoeHNeTaH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3d09cf-47f3-4fa1-93ce-2dc1c5f4b63c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1869', 'Wurzel', 'Nuon', 'yearly', 'termed', 'modes', '##asen', '##事', 'Camer', 'Chlorella']\n",
            "Word Pair: (disease, Krankheit), AP: 0.82\n",
            "One or both of the words not found in vocabulary.\n",
            "['##9', 'propagate', '##authorized', 'acetylcholine', 'Leistungen', '##bungen', 'festiv', 'pathologies', 'lateinische', 'sant']\n",
            "Word Pair: (Bronze, Bronze), AP: 1.00\n",
            "['Komp', '##führer', 'drainage', 'geeigneten', '##gegeben', 'CD4', 'Modern', 'wrest', 'Allgemeinen', 'Mundhöhle']\n",
            "Word Pair: (humanity, Menschheit), AP: 0.99\n",
            "['##aea', '##ubles', 'neben', 'Off', 'attendance', 'helfen', 'pancreatic', 'peel', 'Handbuch', 'viele']\n",
            "Word Pair: (cook, Kochen), AP: 0.58\n",
            "Mean Average Precision (MAP): 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from here on, these code lines are for understanding the data and extracting data statistics"
      ],
      "metadata": {
        "id": "nEfv9H6KrUfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code calculates the total word count in both english and german text files\n",
        "\n"
      ],
      "metadata": {
        "id": "Qu-Ug47Ergv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_words_in_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            words = content.split()\n",
        "            return len(words)\n",
        "    except FileNotFoundError:\n",
        "        return 0\n",
        "\n",
        "def count_words_in_folder(folder_path):\n",
        "    folder_word_count = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                folder_word_count += count_words_in_file(file_path)\n",
        "    return folder_word_count\n",
        "\n",
        "def count_words_in_folders(root_folder):\n",
        "    total_word_count = 0\n",
        "    for sub_folder in os.listdir(root_folder):\n",
        "        sub_folder_path = os.path.join(root_folder, sub_folder)\n",
        "        if os.path.isdir(sub_folder_path):\n",
        "            folder_word_count = count_words_in_folder(sub_folder_path)\n",
        "            total_word_count += folder_word_count\n",
        "            print(f\"Folder: {sub_folder}, Word Count: {folder_word_count}\")\n",
        "    print(f\"Total Word Count for All Files: {total_word_count}\")\n",
        "\n",
        "# Specify the root folder containing your files\n",
        "root_folder = '/content/drive/MyDrive/NLP_2/english'\n",
        "\n",
        "count_words_in_folders(root_folder)\n",
        "\n",
        "\n",
        "##### Results output ######\n",
        "\n",
        "# Folder: history_de, Word Count: 10358\n",
        "# Folder: disease_de, Word Count: 85191\n",
        "# Folder: tech_de, Word Count: 23498\n",
        "# Total Word Count for All Files: 119047"
      ],
      "metadata": {
        "id": "OvKG1XewZUpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b25a95-2c9b-43b1-d69e-84400a9d0dcf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder: history_en, Word Count: 132870\n",
            "Folder: disease_en, Word Count: 257739\n",
            "Folder: tech_en, Word Count: 33588\n",
            "Total Word Count for All Files: 424197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "metadata": {
        "id": "hzdYBN9kBaBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "pE58GHY2FDn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Loading English and German language models from spacy\n",
        "nlp_english = spacy.load(\"en_core_web_sm\")\n",
        "nlp_german = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "# the `nlp.max_length` can be increased according to the size of corpus\n",
        "nlp_english.max_length = 3000000\n",
        "nlp_german.max_length = 3000000\n",
        "\n",
        "# Function to process a text file and return a set of words, using spacy because it is pretrained on these language and know more words\n",
        "def process_text_file(file_path, nlp_model):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        doc = nlp_model(text)\n",
        "        words = {token.text for token in doc if token.is_alpha}\n",
        "        return words\n",
        "\n",
        "english_file_path = '/content/drive/MyDrive/NLP_2/combined_english.txt'\n",
        "german_file_path = '/content/drive/MyDrive/NLP_2/combined_german.txt'\n",
        "\n",
        "english_words = process_text_file(english_file_path, nlp_english)\n",
        "german_words = process_text_file(german_file_path, nlp_german)\n",
        "\n",
        "# For finding common terminology between English and German text files taking intersection of both files\n",
        "common_terminology = english_words.intersection(german_words)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gO-Yw1vpFPVO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(common_terminology))"
      ],
      "metadata": {
        "id": "ZZ3i2I1kR7sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813c9356-ee1d-4191-a15d-e66fef399eff"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating shared terminology coverage\n",
        "shared_terminology = english_words.intersection(german_words)\n",
        "english_coverage = len(shared_terminology) / len(english_words) * 100\n",
        "german_coverage = len(shared_terminology) / len(german_words) * 100\n",
        "\n",
        "# Printing the shared terminology coverage\n",
        "print(\"Shared Terminology Coverage:\")\n",
        "print(f\"Shared Terminology Count: {len(shared_terminology)} words\")\n",
        "print(f\"English Corpus Coverage: {english_coverage:.2f}%\")\n",
        "print(f\"German Corpus Coverage: {german_coverage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Oj8GhUsAK4pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fde1b2-c507-404d-98fc-891319e68fc3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared Terminology Coverage:\n",
            "Shared Terminology Count: 1853 words\n",
            "English Corpus Coverage: 6.17%\n",
            "German Corpus Coverage: 8.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Aw9heoOqGMP9"
      }
    }
  ]
}